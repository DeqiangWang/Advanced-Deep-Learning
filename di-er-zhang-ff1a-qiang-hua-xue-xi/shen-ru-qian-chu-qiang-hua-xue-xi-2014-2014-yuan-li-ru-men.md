# 推荐序

## 序一

强化学习目标是解决决策最优化问题。所谓**决策最优化**，是指面对特定状态（State，$$S$$），采取什么行动方案（Action，$$A$$），才能使收益最大化（Reward，$$R$$）.

**马尔科夫决策过程**（DPM）是最简单的强化学习的数学模型，有如下原因

1. 状态$$s_t$$的数量有限；
2. 行动方案$$a_t$$的数量有限；
3. 特定状态$$s_t$$的收益$$r_t$$是明确的；
4. $$s_t$$下采取行动$$a_t$$，下一个状态$$s_{t+1}$$不是确定的。

**动态规划**是解决DPM最常用的算法。

强化学习的目标是研究从DPM出发，放松各种限制。

例如$$s_t$$可能是数量庞大或者无限的，或者是连续而非离散的，或者$$s_t$$是只能部分被观察或者不能被观察。再次情况下，强化学习的目标不是寻找最优解，二是寻找次优解。

## 序三

**闭环学习**：采用动态的数据及标签，将数据产生和模型优化通过一定的交互方式结合在一起，将动态反馈信号引入学习过程。

## 序五

强化学习模拟的是人类的一种学习方式，在执行摸个动作或者决策后根据执行效果来获得奖励，通过不断与环境的交互进行学习，最终达到目标。

# 1. 绪论

## 1.2 强化学习可以解决什么问题

**智能决策问题**或者**序贯决策问题**。序贯决策问题是指需要连续不断地做决策，才能实现最终目标的问题。

## 1.3 强化学习如何解决问题

解决方式：智能体通过动作与环境进行交互时，环境会返给智能体一个当前的回报，智能体则根据当前的回报与环境进行交互：有利于实现动作的目标会保留，不利于实现目标的动作会被衰减。

强化学习与监督学习的异同：共同点是两者都需要大量的数据进行训练，但是两者需要的数据类型不同。监督学习需要的是多样化的标签数据，强化学习需要的是带有回报的交互数据。

## ![](/assets/srqcqhxx_1_3_1.png)

## 1.4 强化学习的算法分类及发展趋势

是否需要模型

1. **基于模型的强化学习：**效率更高
2. **无模型的强化学习**：更好的通用性

更新策略和学习方法

1. **基于值函数的强化学习**：学习值函数，策略根据值函数贪心得到
2. **基于直接策略搜索的强化学习**：将策略参数化，学习优化目标的最优参数
3. **AC的方法**：联合使用以上两种方法。

环境返回的回报函数是否已知

1. **正向强化学习**：回报函数已经指定
2. **逆向强化学习**：需要自行学习回报函数

强化学习的发展趋势

1. 强化学习与深度学习的结合更加紧密
2. 强化学习与专业知识的结合更加紧密
3. 强化学习的算法理论分析会更强，算法会更加稳定和高效
4. 强化学习与生物学的结合会更加紧密

# 第一篇：强化学习基础

# 2. 马尔科夫决策过程

## 2.1 马尔科夫决策过程理论讲解

#### 马尔科夫性

**马尔科夫性**：系统的下一个状态$$s_{t+1}$$仅与当前状态$$s_t$$有关，而与之前的状态无关

定义：状态$$s_t$$是马尔科夫的，当且仅当$$P[s_{t+1}|s_{t}]=P[s_{t+1}|s_1,...,s_t]$$。

**马尔科夫随机过程**：随机变量序列中的每个状态都是马尔科夫的。

#### 马尔科夫过程

**马尔科夫过程**是一个二元组\(S, P\), 且满足：S是有限状态集合，P是状态转移概率。

**马尔科夫决策过程**：将动作（策略）和回报考虑在内的马尔科夫过程成为马尔科夫随机过程。

### 马尔科夫决策过程

马尔科夫决策过程由元组$$(S, A, P, R, \gamma)$$描述，其中:

* S为有限的状态集
* A为有限的动作集
* P为状态转移概率
* R为回报函数
* $$\gamma$$为折扣因子，用来计算累积回报

马尔科夫决策过程的的状态转移概率是包含动作的，即$$P^{a}_{ss'}=p[S_{t+1}=s'|S_t=s,A_t=a]$$。

强化学习的目标是给定一个马尔科夫决策过程，寻找**最优策略**。

最优策略：是指状态到动作的映射，策略通常用符号$$\pi$$表示，它是指给定状态$$s$$时，动作集上的一个分布，即
$$
\pi(a|s)=p[A_t=a|S_t=s]
$$
它的含义是：策略$$\pi$$在每个状态$$s$$指定一个动作概率。如果给出的策略$$\pi$$是确定性的，那么策略$$\pi$$在每个状态$$s$$指定一个确定的动作。

累计回报：
$$
G_t=R_{t+1}+\gamma R_{t+2}+...=\sum^{\infty}_{k=0}\gamma^kR_{t+k+1}
$$
















