# 推荐序

## 序一

强化学习目标是解决决策最优化问题。所谓**决策最优化**，是指面对特定状态（State，$$S$$），采取什么行动方案（Action，$$A$$），才能使收益最大化（Reward，$$R$$）.

**马尔科夫决策过程**（DPM）是最简单的强化学习的数学模型，有如下原因

1. 状态$$s_t$$的数量有限；
2. 行动方案$$a_t$$的数量有限；
3. 特定状态$$s_t$$的收益$$r_t$$是明确的；
4. $$s_t$$下采取行动$$a_t$$，下一个状态$$s_{t+1}$$不是确定的。

**动态规划**是解决DPM最常用的算法。

强化学习的目标是研究从DPM出发，放松各种限制。

例如$$s_t$$可能是数量庞大或者无限的，或者是连续而非离散的，或者$$s_t$$是只能部分被观察或者不能被观察。再次情况下，强化学习的目标不是寻找最优解，二是寻找次优解。

## 序三

**闭环学习**：采用动态的数据及标签，将数据产生和模型优化通过一定的交互方式结合在一起，将动态反馈信号引入学习过程。

## 序五

强化学习模拟的是人类的一种学习方式，在执行摸个动作或者决策后根据执行效果来获得奖励，通过不断与环境的交互进行学习，最终达到目标。

# 1. 绪论

## 1.2 强化学习可以解决什么问题

**智能决策问题**或者**序贯决策问题**。序贯决策问题是指需要连续不断地做决策，才能实现最终目标的问题。

## 1.3 强化学习如何解决问题

解决方式：智能体通过动作与环境进行交互时，环境会返给智能体一个当前的回报，智能体则根据当前的回报与环境进行交互：有利于实现动作的目标会保留，不利于实现目标的动作会被衰减。

强化学习与监督学习的异同：共同点是两者都需要大量的数据进行训练，但是两者需要的数据类型不同。监督学习需要的是多样化的标签数据，强化学习需要的是带有回报的交互数据。

## ![](/assets/srqcqhxx_1_3_1.png)

## 1.4 强化学习的算法分类及发展趋势

是否需要模型

1. **基于模型的强化学习：**效率更高
2. **无模型的强化学习**：更好的通用性

更新策略和学习方法

1. **基于值函数的强化学习**：学习值函数，策略根据值函数贪心得到
2. **基于直接策略搜索的强化学习**：将策略参数化，学习优化目标的最优参数
3. **AC的方法**：联合使用以上两种方法。

环境返回的回报函数是否已知

1. **正向强化学习**：回报函数已经指定
2. **逆向强化学习**：需要自行学习回报函数



