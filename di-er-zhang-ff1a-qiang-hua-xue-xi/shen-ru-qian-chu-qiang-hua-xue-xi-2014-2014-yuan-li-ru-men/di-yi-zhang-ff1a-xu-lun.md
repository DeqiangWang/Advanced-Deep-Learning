# 1. 绪论

## 1.2 强化学习可以解决什么问题

**智能决策问题**或者**序贯决策问题**。序贯决策问题是指需要连续不断地做决策，才能实现最终目标的问题。

## 1.3 强化学习如何解决问题

解决方式：智能体通过动作与环境进行交互时，环境会返给智能体一个当前的回报，智能体则根据当前的回报与环境进行交互：有利于实现动作的目标会保留，不利于实现目标的动作会被衰减。

强化学习与监督学习的异同：共同点是两者都需要大量的数据进行训练，但是两者需要的数据类型不同。监督学习需要的是多样化的标签数据，强化学习需要的是带有回报的交互数据。

![](/assets/srqcqhxx_1_3_1.png)

## 1.4 强化学习的算法分类及发展趋势

是否需要模型

1. **基于模型的强化学习：**效率更高
2. **无模型的强化学习**：更好的通用性

更新策略和学习方法

1. **基于值函数的强化学习**：学习值函数，策略根据值函数贪心得到
2. **基于直接策略搜索的强化学习**：将策略参数化，学习优化目标的最优参数
3. **AC的方法**：联合使用以上两种方法。

环境返回的回报函数是否已知

1. **正向强化学习**：回报函数已经指定
2. **逆向强化学习**：需要自行学习回报函数

强化学习的发展趋势

1. 强化学习与深度学习的结合更加紧密
2. 强化学习与专业知识的结合更加紧密
3. 强化学习的算法理论分析会更强，算法会更加稳定和高效
4. 强化学习与生物学的结合会更加紧密



