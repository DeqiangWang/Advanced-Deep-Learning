# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

## 前言

接触NLP领域不久，也来凑热点学习一下BERT，文章中可能存在些许错误，权当学习笔记了。

